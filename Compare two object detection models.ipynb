{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare two object detection models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import object_detections dataset\n",
    "ground_truth = pd.read_csv('data/gt_boxes.csv')\n",
    "inference_1 = pd.read_csv('data/model_1.csv')\n",
    "inference_2 = pd.read_csv('data/model_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some possible metric(s) to compare two multi-class object detection models\n",
    "def bb_intersection_over_union(boxA, boxB):\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = abs(max((xB - xA, 0)) * max((yB - yA), 0))\n",
    "    if interArea == 0:\n",
    "        return 0\n",
    "    # compute the area of both the prediction and ground-truth rectangles\n",
    "    boxAArea = abs((boxA[2] - boxA[0]) * (boxA[3] - boxA[1]))\n",
    "    boxBArea = abs((boxB[2] - boxB[0]) * (boxB[3] - boxB[1]))\n",
    "\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "    # return the intersection over union value\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the IOU, Accuracy and Weighted Accuracy to zero for both inferences.\n",
    "stats = pd.DataFrame(columns=['iou_1', 'iou_2', 'acc_1', 'acc_2', 'acc_w_1', 'acc_w_2'])\n",
    "\n",
    "length = len(ground_truth)\n",
    "for idx in range(length):\n",
    "    data = {'iou_1': 0, 'iou_2': 0, 'acc_1': 0, 'acc_2': 0, 'acc_w_1': 0, 'acc_w_2': 0}\n",
    "    image_id = ground_truth.loc[idx]['image_id']\n",
    "    label = ground_truth.loc[idx]['label']\n",
    "    ground_truth_bbox = [ground_truth.loc[idx]['xmin'], ground_truth.loc[idx]['ymin'], ground_truth.loc[idx]['xmax'], ground_truth.loc[idx]['ymax']]\n",
    "    # Corresponding image_id and label\n",
    "    inference_1_match = inference_1[(inference_1['image_id'] == image_id) & (inference_1['label'] == label)]\n",
    "    inference_2_match = inference_2[(inference_1['image_id'] == image_id) & (inference_2['label'] == label)]\n",
    "    \n",
    "    if not inference_1_match.empty:\n",
    "        # IOU\n",
    "        inferences_1_bbox = [inference_1_match.loc[idx]['xmin'], inference_1_match.loc[idx]['ymin'], inference_1_match.loc[idx]['xmax'], inference_1_match.loc[idx]['ymax']]\n",
    "        data['iou_1'] = bb_intersection_over_union(ground_truth_bbox, inferences_1_bbox)\n",
    "        # Accuracy\n",
    "        data['acc_1'] = 1 if ground_truth.loc[idx]['label'] == inference_1_match.loc[idx]['label'] else 0\n",
    "        # Weighted accuracy\n",
    "        data['acc_w_1'] = data['acc_1'] * inference_1_match.loc[idx]['score']\n",
    "        \n",
    "    if not inference_2_match.empty:\n",
    "        # IOU\n",
    "        inferences_2_bbox = [inference_2_match.loc[idx]['xmin'], inference_2_match.loc[idx]['ymin'], inference_2_match.loc[idx]['xmax'], inference_2_match.loc[idx]['ymax']]\n",
    "        data['iou_2'] = bb_intersection_over_union(ground_truth_bbox, inferences_2_bbox)\n",
    "        # Accuracy\n",
    "        data['acc_2'] = 1 if ground_truth.loc[idx]['label'] == inference_2_match.loc[idx]['label'] else 0\n",
    "        # Weighted accuracy\n",
    "        data['acc_w_2'] = data['acc_2'] * inference_2_match.loc[idx]['score']\n",
    "        \n",
    "    # Append\n",
    "    stats = stats.append(data, ignore_index=True)\n",
    "    \n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing two models\n",
    "print('Inference 1:')\n",
    "print(f\"IOU: {float(stats[['iou_1']].mean()):.2f}\")\n",
    "print(f\"Accuracy: {(float(stats['acc_1'].count()) / length):.2f}\")\n",
    "print(f\"Weighted Accuracy: {float(stats[['acc_w_1']].mean()):.2f}\")\n",
    "\n",
    "print('')\n",
    "print('Inference 2:')\n",
    "print(f\"IOU: {float(stats[['iou_2']].mean()):.2f}\")\n",
    "print(f\"Accuracy: {(float(stats['acc_2'].count()) / length):.2f}\")\n",
    "print(f\"Weighted Accuracy: {float(stats[['acc_w_2']].mean()):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
